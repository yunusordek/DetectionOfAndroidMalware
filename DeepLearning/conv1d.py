

# Code to read csv file into Colaboratory:
!pip install -U -q PyDrive
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials
# Authenticate and create the PyDrive client.
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

link="Your google drive csv location link"

fluff, id = link.split('=')
print (id) # Verify that you have everything after '='

import pandas as pd

downloaded = drive.CreateFile({'id':id}) 
downloaded.GetContentFile('new_all.csv')  
data = pd.read_csv('new_all.csv')

import scipy as sp
from sklearn.model_selection import train_test_split
from keras.preprocessing import sequence
import numpy as np

x=data.iloc[:,0:82]
y=data.iloc[:,82:83]
x=x.values
y=y.values

x_eğitim, x_test,y_eğitim, y_test = train_test_split(x, y, test_size = 0.2, random_state = 0)

from keras.layers import Dense, Embedding, Flatten, SimpleRNN
from keras.models import Sequential
from keras.layers import Conv1D, GlobalAveragePooling1D, MaxPooling1D
from keras.layers import GlobalMaxPooling1D
from keras.layers import Dense, Dropout, Activation
import matplotlib.pyplot as plt

"""
model = Sequential()
model.add(Conv1D(filters=16, kernel_size=32, activation='relu', data_format='channels_first'))
model.add(MaxPooling1D())
model.add(MaxPooling1D(pool_size=5))
#model.add(Conv1D(filters=16, kernel_size=35, activation='relu',batch_input_shape=(None, 64, 1), data_format='channels_first'))
model.add(GlobalAveragePooling1D())
model.add(Dense(1, activation='sigmoid'))
"""

max_features = 5000
maxlen = 82
batch_size = 32
embedding_dims = 50
filters = 250
kernel_size = 3
hidden_dims = 250
epochs = 2

model = Sequential()

# we start off with an efficient embedding layer which maps
# our vocab indices into embedding_dims dimensions
model.add(Embedding(max_features,
                    embedding_dims,
                    input_length=maxlen))
model.add(Dropout(0.5))

# we add a Convolution1D, which will learn filters
# word group filters of size filter_length:
model.add(Conv1D(filters,
                 kernel_size,
                 padding='valid',
                 activation='relu',
                 strides=1))
# we use max pooling:
model.add(GlobalMaxPooling1D())

# We add a vanilla hidden layer:
model.add(Dense(hidden_dims))
model.add(Dropout(0.3))
model.add(Activation('relu'))

# We project onto a single unit output layer, and squash it with a sigmoid:
model.add(Dense(1))
model.add(Activation('sigmoid'))

model.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

histroy=model.fit(x_eğitim, y_eğitim,
          batch_size=32,
          epochs=10,
          validation_data=(x_test, y_test))

import matplotlib.pyplot as plt
#Plot training & validation accuracy values
plt.plot(histroy.history['acc'])
plt.plot(histroy.history['val_acc'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Eğitim', 'Test'], loc='upper left')
plt.show()

# Plot training & validation loss values
plt.plot(histroy.history['loss'])
plt.plot(histroy.history['val_loss'])
plt.title('Model Kayıp')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Eğitim', 'Test'], loc='upper left')
plt.show()